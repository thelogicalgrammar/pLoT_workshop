{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03d9bbe-49d3-4ad8-8e48-26d2f4133977",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce1982d-aab0-42a2-84d9-884501f19230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in colab!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # in colab\n",
    "    import google.colab\n",
    "    print('In colab, downloading LOTlib3')\n",
    "    !git clone https://github.com/thelogicalgrammar/LOTlib3\n",
    "except:\n",
    "    # not in colab\n",
    "    print('Not in colab!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e80984-322a-4bd6-b36a-f0dd0ea98c10",
   "metadata": {},
   "source": [
    "> **NOTE** This notebook is a reworked version of Steven Piantadosi's Tutorial to LOTlib3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e326d728-7681-46c6-aa7a-27499bb94efb",
   "metadata": {},
   "source": [
    "LOTlib3 is a library for inferring compositions of functions from observations of their inputs and outputs or simply their outputs if there is no input. This tutorial will introduce a very simple problem and how it can be solved in LOTlib3. \n",
    "\n",
    "Suppose that you know basic arithmetic operations (called \"primitives\") like addition (+), subtraction (-), multiplication (*) and division (/). You observe a number which has been constructed using these operations, and wish to infer which operations were used. We'll assume that you observe the single number `12` and then do Bayesian inference in order to discover which operations occurred. For instance, 12 may be written as `(1+1) * 6`, involving an addition, a multiplication, two uses of 1, and one use of 6. Or it may have been written as ` 1 + 11`, or `2 * 3 * 2`, etc. There are lots of other ways.  Which one is correct?  Well any composition of addition, subtraction, multiplication, etc... that outputs a 12 would be consistent with the data.  However, some might seem more plausible than others.  For instance it might be unlikely the 12 was generated as 12+(1-1+1-1+1-1+1-1+1-1+1-1+1-1+1-1) as it seems a rather obtuse way to have arrived as 12.  LOTLib3 is designed to help you make sensible decisions about which compositions of functions likely explain a particular set of outputs or input-output pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f751693-ec85-4523-a791-a877ea7d2158",
   "metadata": {},
   "source": [
    "## Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f45e2a-78f2-4cdc-aa78-7d8f00e98f15",
   "metadata": {},
   "source": [
    "The general strategy of LOTlib3 models is to specify a space of possible compositions using a grammar. The grammar is actually a probabilistic context free grammar (with one small modification described below) that specifies a prior distribution on trees, or equivalently compositional structures like (1+1)*6, 2+2+2+2+2+2, (1+1)+(2*5), etc. If this is unfamiliar, the wiki on [PCFGs](https://en.wikipedia.org/wiki/Stochastic_context-free_grammar) would be useful to read first. \n",
    "\n",
    "However, the best way to understand the grammar is as a way of specifying a program: any expansion of the grammar \"renders\" into a python program, whose code can then be evaluated. This will be made more concrete later.\n",
    "\n",
    "For now, here is how we can construct a simple example grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69318b-8ea3-4660-9b39-c5ebe397d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LOTlib3.Grammar import Grammar\n",
    "\n",
    "# Define a grammar object\n",
    "# Defaultly this has a start symbol called 'START' but we want to call \n",
    "# it 'EXPR'\n",
    "grammar = Grammar(start='EXPR')\n",
    "\n",
    "# Define some operations\n",
    "grammar.add_rule('EXPR', '(%s + %s)', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', '(%s * %s)', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', '(float(%s) / float(%s))', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', '(-%s)', ['EXPR'], 1.0)\n",
    "\n",
    "# And define some numbers. We'll give them a 1/(n^2) probability\n",
    "for n in range(1,10):\n",
    "    grammar.add_rule('EXPR', str(n), None, 10.0/(n**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0fb53-75ee-4d37-9885-13f32b766796",
   "metadata": {},
   "source": [
    "A few things to note here. The grammar rules have the format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb9cdf-98ce-4762-94ef-179f0ecdf351",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar.add_rule( <NONTERMINAL>, <FUNCTION>, <ARGUMENTS>, <PROBABILITY>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ae288-f6e5-4d97-b0db-b25ea612c7c4",
   "metadata": {},
   "source": [
    "where <NONTERMINAL> says what nonterminal this rule is expanding. In this example, there is only one kind of nonterminal, an expression (denoted EXPR). The <FUNCTION> argument is the function that this rule represents. These are strings that name defined functions in LOTlib3, but they can also be strings (as here) where the <ARGUMENTS> get substituted in via string substitution (so for instance, \"(%s+%s)\" can be viewed as the function `lambda x,y: eval(\"(%s+%s)\"%(x,y)))`. The arguments are a list of the arguments to the function. Note that the <FUNCTION> string can be pretty complex. For division, we have `(float(%s) / float(%s))`, which forces the function to use floating point division, not python's default. \n",
    "\n",
    "If the <FUNCTION> is a terminal that does not take arguments (as in the numbers 0..9), the <ARGUMENTS> part of a rule should be None. Note that None is very different from an empty list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99deb0-2832-4bf9-ab2a-26a14bd3bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar.add_rule('EXPR', 'hello', None, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd98508-a26a-40d6-8fb1-1aca0092e20d",
   "metadata": {},
   "source": [
    "renders into the program \"hello\" but "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce844a5a-96fe-49ef-b1ae-c7079a621cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar.add_rule('EXPR', 'hello', [], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd774c-64b2-4383-9fc2-3b6458ed03ab",
   "metadata": {},
   "source": [
    "renders into \"hello()\". \n",
    "\n",
    "\n",
    "The production probabilities are, for now, fixed. Note that the numbers have probabilities proportional to 1/n**2.  This means that is prefers smaller numbers. But we also multiplied those values by 10, making them as a group much more likely than other operations. This is important because the PCFG has to define a proper probability distribution. This means that a nonterminal must have a probability of 1 of eventually leading to a terminal (a terminal is any rule with `None` as its <ARGUMENTS>). The easiest way to ensure this is to upweight the probabilities on terminals so they are more likely to be sampled.  We might have been ok in this simple case because we are only dealing with the numbers 1-9 but as the number of terminal possibilities increases eventually the probability of sampling one would become very small compared to repeatedly sampling the more complex expressions that have weight 1.0.\n",
    "    \n",
    "We can see some productions from this grammar if we call Grammar.generate. We will also show the probability of this tree according to the grammar, which is computed by renormalizing the <PROBABILITY> values of each rule when expanding each nonterminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb865d4-731c-4b69-86ce-a527d92143f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in xrange(100):\n",
    "    t = grammar.generate()\n",
    "    print grammar.log_probability(t), t "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b806b-ca65-4ef7-aa64-dc45e460c449",
   "metadata": {},
   "source": [
    "As you can see, the longer/bigger trees have lower (more negative) probabilities, implementing essentially a simplicity bias. These PCFG probabilities will often be our prior for Bayesian inference. \n",
    "\n",
    "Note that even though each `t` is a tree (a hierarchy of LOTlib3.FunctionNodes), it renders nicely above as a string. This is defaultly how expressions are evaluated in python. But we can see more of the internal structure using `t.fullprint()`, which shows the nonterminals, functions, and arguments at each level:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271652d-537c-4a6a-975a-437ddf7ffd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = grammar.generate()\n",
    "t.fullprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b101eef5-f97c-4033-99de-cd4b3cb67e4c",
   "metadata": {},
   "source": [
    "There is a column in this output that should say 'None' for each row of the output.  It is ok to ignore that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd9f473-b0d6-4835-bd35-a7ebbcc06ed5",
   "metadata": {},
   "source": [
    "## Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a24e4-01a6-4af6-acd3-a8fc7f23838d",
   "metadata": {},
   "source": [
    "The grammar nicely specifies a space of expressions, but LOTlib3 needs a \"hypothesis\" to perform inference. In LOTlib3, hypotheses must define functions for computing priors, computing the likelihood of data, and implementing proposals in order for MCMC to work. In most cases, a hypothesis will represent a single production from the grammar.\n",
    "\n",
    "Fortunately, for our purposes, there is a simple hypothesis class that it built-in to LOTlib3 which defaultly implements these. Let's just use it here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c56a7-401f-4a1f-a677-0071516e1bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from LOTlib3.Hypotheses.LOTHypothesis import LOTHypothesis\n",
    "\n",
    "# define a \n",
    "class MyHypothesis(LOTHypothesis):\n",
    "    def __init__(self, **kwargs):\n",
    "        LOTHypothesis.__init__(self, grammar=grammar, display=\"lambda: %s\", **kwargs)\n",
    "\n",
    "    def compute_single_likelihood(self, datum):\n",
    "        if self(*datum.input) == datum.output:\n",
    "            return log((1.0-datum.alpha)/100. + datum.alpha)\n",
    "        else:\n",
    "            return log((1.0-datum.alpha)/100.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e721f2-c2d0-4fd8-bddb-5332bae91eec",
   "metadata": {},
   "source": [
    "There are a few things going on here. First, we import LOTHypothesis and use that to define the new class `MyHypothesis`. LOTHypothesis defines `compute_prior()` and `compute_likelihood(data)`--more about these later. We define the initializer `__init__`. We overwrite the LOTHypothesis default and specify that the grammar we want is the one defined above. LOTHypotheses also defaultly take an argument called `x` (more on this later), but for now we want our hypothesis to be a function of no arguments. When we convert the value of the hypothesis into a string, it will get substituted into the `display` keyword. Here, `display=\"lambda: %s\"` meaning that the hypothesis will be displayed and also evaled in python as appearing after a lambda.\n",
    "\n",
    "Essentially, `compute_likelihood` maps `compute_single_likelihood` over a list of data (treating each as IID conditioned on the hypothesis). So when we want to define how the likelihood works, we typically want to overwrite `compute_single_likelihood` as we have above. In this function, we expect an input `datum` with attributes `input`, `output`, and `alpha`. The LOTHypothesis `self` can be viewed as a function (here, one with no arguments) and so it can be called on `datum.input`. The likelihood this defines is one in which we generate a random number from 1..100 with probability `1-datum.alpha` and the correct number with probability `datum.alpha`. Thus, when the hypothesis returns the correct value (e.g. `self(*datum.input) == datum.output`) we must add these quantities to get the total probability of producing the data. When it does not, we must return only the former. LOTlib3.Hypotheses.Likelihoods defines a number of other standard likelihoods, including the most commonly used  one, `BinaryLikelihood`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6182b-7e4c-4ef3-b53b-584d4ac039c1",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df26cd4-1c05-42c3-9b67-0f1f501cc240",
   "metadata": {},
   "source": [
    "Given that our hypothesis wants those kinds of data, we can then create data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef27541-4782-4ae6-a252-a9e358e8dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LOTlib3.DataAndObjects import FunctionData\n",
    "data = [ FunctionData(input=[], output=12, alpha=0.95) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db4f0c1-c3ba-4f50-9984-e3252fdd8ab4",
   "metadata": {},
   "source": [
    "Note here that the most natural form of data is a list--even if it is only a single element--where each element, a datum, gets passed to `compute_single_likelihood`. The data here specifies the input, output, and noise value `alpha`. Note that even though `alpha` could live as an attribute of hypotheses, it makes most sense to view it as a known part of the data. Here input is a empty list because there is no input!  Remember the example we started with was explaining a single output (the number 12).  Keep reading to understand examples with an input below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de259d-3a9b-4248-8c23-774ce95f4a28",
   "metadata": {},
   "source": [
    "## Making hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd59c5e-b499-45b2-aa82-3244ccdbfce1",
   "metadata": {},
   "source": [
    "We may now use our definition of a hypothesis to make one. If we call the initializer without a `value` keyword, LOTHypothesis just samples it from the given grammar: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7bea97-d4f3-4ce4-b3ab-358252641c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = MyHypothesis()\n",
    "print(h.compute_prior(), h.compute_likelihood(data), h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c88f30-5b11-4e38-9de4-9a5eface125e",
   "metadata": {},
   "source": [
    "Even better, `MyHypothesis` also inherits a `compute_posterior` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a0da1-f145-4619-9a6c-5fdbb44c724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h.compute_posterior(data), h.compute_prior(), h.compute_likelihood(data), h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea5f30-65f5-4b28-a13b-6eafab3c097a",
   "metadata": {},
   "source": [
    "For convenience, when `compute_posterior` is called, it sets attributes on `h` for the prior, likelihood, and posterior (score):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb8772-dd91-4bcd-9887-b4519a7298f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = MyHypothesis()\n",
    "h.compute_posterior(data)\n",
    "print(h.posterior_score, h.prior, h.likelihood, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a72a3-3627-4f8c-b86c-4f65015f5f4e",
   "metadata": {},
   "source": [
    "## Running MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d49d3f-e2f8-4d8e-97b4-aba99eca01d7",
   "metadata": {},
   "source": [
    "We are almost there. We have define a grammar and a hypothesis which uses the grammar to define a prior, and custom code to define a likelihood. LOTlib3's main claim to fame is that we can simply import MCMC routines and do inference over the space defined by the grammar. It's very easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a52cc83-9fa4-4e34-bfcf-4ba3bf43fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from LOTlib3.Samplers.MetropolisHastings import MetropolisHastingsSampler\n",
    "    \n",
    "    # define a \"starting hypothesis\". This one is essentially copied by \n",
    "    # all proposers, so the sampler doesn't need to know its type or anything. \n",
    "    h0 = MyHypothesis()\n",
    "    \n",
    "    # Now use the sampler like an iterator. In MetropolisHastingsSampler, compute_posterior gets called\n",
    "    # so when we have an h, we can get its prior and likelihood\n",
    "    for h in MetropolisHastingsSampler(h0, data, steps=100):\n",
    "        print(h.posterior_score, h.prior, h.likelihood, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029f532e-b536-414d-a868-9d2218d4dd42",
   "metadata": {},
   "source": [
    "That probably went by pretty fast and also generated error messages! Here's another thing we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6a018-6c49-465f-9632-e6dbb05d9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "    h0 = MyHypothesis()\n",
    "    \n",
    "    from collections import Counter\n",
    "\n",
    "    count = Counter()\n",
    "    for h in MetropolisHastingsSampler(h0, data, steps=1000):\n",
    "        count[h] += 1\n",
    "    \n",
    "    for h in sorted(count.keys(), key=lambda x: count[x]):\n",
    "        print(count[h], h.posterior_score, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae99706-6e6a-4ab8-a809-b6d60ad8683b",
   "metadata": {},
   "source": [
    "LOTlib3 hypotheses are required to hash nicely, meaning that they can be saved or put into dictionaries and sets like this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e3371b-5982-4024-b99b-511ad32143dd",
   "metadata": {},
   "source": [
    "## Making our hypothesis class more robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d4b76-c40e-4720-a1aa-ef9941518c2e",
   "metadata": {},
   "source": [
    "As you might have noticed we did not allow the number 0 as a terminal value in our grammar.  This is because it ran the risk of a divide by zero error.  However, we might like to include all the numbers between zero and nine.  So let's change that one part of our grammar definiton by replacing the lines like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b93b01a-9a61-4879-976f-6d4970330fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And define some numbers. We'll give them a 1/((n+1)^2) probability\n",
    "    for n in range(10):\n",
    "        grammar.add_rule('EXPR', str(n), None, 10.0/((n+1)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6b8f1-3b83-4131-b5ea-c2fe658ff1e7",
   "metadata": {},
   "source": [
    "But how do we deal with the fact that we might get a expression with a zero in the demoninator?  This will generate a divide by zero error when evaluated by the Python interpreter.\n",
    "\n",
    "Fortunately, we can hack our hypothesis class to address this by catching the exception. A smart way to do this is to override `__call__` and return an appropriate value when such an error occurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be0df2-8b9e-4f10-bbe6-386aabea63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from math import log\n",
    "    from LOTlib3.Hypotheses.LOTHypothesis import LOTHypothesis\n",
    "\n",
    "    class MyHypothesis(LOTHypothesis):\n",
    "        def __init__(self, **kwargs):\n",
    "            LOTHypothesis.__init__(self, grammar=grammar, display=\"lambda: %s\", **kwargs)\n",
    "            \n",
    "        def __call__(self, *args):\n",
    "            try:\n",
    "                # try to do it from the superclass\n",
    "                return LOTHypothesis.__call__(self, *args)\n",
    "            except ZeroDivisionError:\n",
    "                # and if we get an error, return nan\n",
    "                return float(\"nan\")\n",
    "    \n",
    "        def compute_single_likelihood(self, datum):\n",
    "            if self(*datum.input) == datum.output:\n",
    "                return log((1.0-datum.alpha)/100. + datum.alpha)\n",
    "            else:\n",
    "                return log((1.0-datum.alpha)/100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40847d53-ada4-415a-9f3a-3b965dcbde94",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting serious about running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416890d-f165-491c-9c88-ed4672303a25",
   "metadata": {},
   "source": [
    "Now with more robust code, we can run the `Counter` code above for longer and get a better picture of the posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26024f-2962-406e-9df6-1cac5f2efb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = MyHypothesis()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# run a bit, counting how often we get each hypothesis\n",
    "count = Counter()\n",
    "for h in MetropolisHastingsSampler(h0, data, steps=100000):\n",
    "    count[h] += 1\n",
    "\n",
    "# print the counts and the posteriors\n",
    "for h in sorted(count.keys(), key=lambda x: count[x]):\n",
    "    print(count[h], h.posterior_score, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7cd08-6a33-4688-8166-d92f8457e04c",
   "metadata": {},
   "source": [
    "If our sampler is working correctly, it should be the case that the time average of the sampler (the `h`es from the for loop) should approximate the posterior distribution (e.g. their re-normalized scores). Let's use this code to see if that's true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11efed79-017b-41b3-9754-c12fbe2284bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscellaneous stores a number of useful functions. Here, we need logsumexp, which will\n",
    "# compute the normalizing constant for posterior_scores when they are in log space\n",
    "from LOTlib3.Miscellaneous import logsumexp \n",
    "from numpy import exp # but things that are handy in numpy are not duplicated (usually)\n",
    "\n",
    "# get a list of all the hypotheses we found. This is necessary because we need a fixed order,\n",
    "# which count.keys() does not guarantee unless we make a new variable. \n",
    "hypotheses = count.keys() \n",
    "\n",
    "# first convert posterior_scores to probabilities. To this, we'll use a simple hack of \n",
    "# renormalizing the psoterior_scores that we found. This is a better estimator of each hypothesis'\n",
    "# probability than the counts from the sampler\n",
    "z = logsumexp([h.posterior_score for h in hypotheses])\n",
    "\n",
    "posterior_probabilities = [ exp(h.posterior_score - z) for h in hypotheses ]\n",
    "\n",
    "# and compute the probabilities over the sampler run\n",
    "cntz = sum(count.values())    \n",
    "sampler_counts          = [ float(count[h])/cntz for h in hypotheses ] \n",
    "\n",
    "## and let's just make a simple plot\n",
    "import matplotlib.pyplot as pyplot\n",
    "fig = pyplot.figure()\n",
    "plt = fig.add_subplot(1,1,1)\n",
    "plt.scatter(posterior_probabilities, sampler_counts)\n",
    "plt.plot([0,1], [0,1], color='red')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8acfeb1-4f71-4df2-b028-40e084427430",
   "metadata": {},
   "source": [
    "## Primitives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0000d840-47fb-426a-a490-f04706dd0583",
   "metadata": {},
   "source": [
    "LOTlib3 also builds in a number of primitive operations, which live in LOTlib3.Primitives. When these are supplied as the <FUNCTION> in grammar rule, they act as functions that get called. **By convention, LOTlib3 internal primitives end in an underscore**. Here is an example equivalent to the grammar above, but using LOTlib3 function calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5adc956-ec23-4a5d-849f-3ac62d92c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar.add_rule('EXPR', 'plus_', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', 'times_', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', 'divide_(float(%s),float(%s))', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', 'neg_', ['EXPR'], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e78f1-8ad7-48c4-b8fa-f6862d50803f",
   "metadata": {},
   "source": [
    "Note that when these are rendered into strings, they appear as function calls (and not just string substitutions to make python code) as in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ccf04a-5f79-4347-a1db-22c54ce3aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_(plus_(x,neg_(1)), plus_(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5949c-f9ae-488b-a1fc-a5b25fa44d08",
   "metadata": {},
   "source": [
    "There are many functions built-in to python, including a number of operations for manipulating sets, numbers, and logic. The code for `divide_` shows that it does not cast its arguments to floats: here we have to do so using string substitution as above.  For simple thing it is up to you to decide if you like to use the built in python operators (e.g. '+') or the LOTLib3 primitive ('plus_') as they are basically the same.\n",
    "\n",
    "You can also create new primitives which extends the functionality of LOTlib3 in many interesting way. To make a custom function accessible to LOTlib3's evaluator, use the `@primitive` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a66bac-61e8-41a3-9a2e-3d2a8950c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LOTlib3.Eval import primitive\n",
    "\n",
    "@primitive\n",
    "def my_stupid_primitive_(x):\n",
    "    return x+90253"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf8728b-daaa-49e9-81f2-9235500312f0",
   "metadata": {},
   "source": [
    "Now if you use `my_stupid_primitive_` in a grammar rule, it can be \"run\" just like any normal python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86751ba-6735-49dc-a7fa-999123b2d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar.add_rule('EXPR', 'my_stupid_primitive_', ['EXPR'], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7114e-45bc-4a65-88c2-ef9735309f19",
   "metadata": {},
   "source": [
    "It is generally more friendly to give it an underscore to make sure it's not confused for a normal python function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5baf0b-9db8-49cd-92a0-ad026d5474c8",
   "metadata": {},
   "source": [
    "## Nonterminals in the grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5df863-a19b-4613-842e-e09742d3e9e6",
   "metadata": {},
   "source": [
    "It may not have been obvious in the above examples, but the <NONTERMINAL> part of each grammar rule can be viewed as specifying the *return type* of the function that rule correspond to, while the <ARGUMENTS> can be viewed as the types of the arguments. Thus, what a grammar mainly does is ensure that the primitives all get arguments of the correct types. \n",
    "\n",
    "Let's see another example: suppose we had two kinds of things: booleans (BOOL) and numbers (EXPR). We might write a grammar like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0341d476-9f2a-4bbd-a958-f49d6b6e6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar.add_rule('EXPR', 'plus_', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', 'times_', ['EXPR', 'EXPR'], 1.0)\n",
    "\n",
    "# Use something that renders into if statements in real python\n",
    "grammar.add_rule('EXPR', '(%s if %s else %s)', ['EXPR', 'BOOL', 'EXPR'], 1.0)\n",
    "\n",
    "# Now how do we get a boolean?\n",
    "grammar.add_rule('BOOL', '(%s > %s)', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('BOOL', '(%s >= %s)', ['EXPR', 'EXPR'], 1.0)\n",
    "\n",
    "# And a terminal\n",
    "grammar.add_rule('EXPR', '1', 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7cb52-8e49-4bb1-a261-548dce4f966b",
   "metadata": {},
   "source": [
    "Here, the second argument to the `if` line must be a BOOL, and so we have given LOTlib3 a way to create code that returns BOOLs. It just happens that this code is a comparison of numbers, or EXPRs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa1643-f151-4c9c-ad6b-c0045ed5d9e6",
   "metadata": {},
   "source": [
    "## The best hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd845a-7736-4cae-93c8-1394162d40ce",
   "metadata": {},
   "source": [
    "Very often, models in LOTlib3 approximate the full posterior distribution P(H|D) using the highest posterior hypotheses. There are two main ways to do this. One is a class named `LOTlib3.TopN` which acts like a set--you can add to it, but it keeps only the ones with highest posterior_score (or whatever \"key\" is set). It will return them to you in a sorted order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91251e3d-e4cc-443c-9e2f-90a7fc33d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LOTlib3.TopN import TopN\n",
    "\n",
    "tn = TopN(N=10) # store the top N\n",
    "\n",
    "h0 = MyHypothesis()\n",
    "\n",
    "for h in MetropolisHastingsSampler(h0, data, steps=10000):\n",
    "    tn.add(h)\n",
    "\n",
    "for h in tn.get_all(sorted=True):\n",
    "    print(h.posterior_score, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183fca3d-9c16-4eb0-9107-ea05045030d6",
   "metadata": {},
   "source": [
    "There's also a friendly way to interface with `TopN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b1d26-c780-4980-bdc2-e81b2bec6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in MetropolisHastingsSampler(h0, data, steps=10000):\n",
    "    tn << h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3550446-2f06-4b09-999c-300101eb29ea",
   "metadata": {},
   "source": [
    "You might notice running this that productions the results in the output 12 are given high posterior scores.  However, it still might be the case that very simple and short programs like just a single number output (like 0) are still the \"top\" hypothesis.  This shows the importance of thinking about the nature of your prior and the likelihood function for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b7626-4818-4431-addb-17c00ffcf1a0",
   "metadata": {},
   "source": [
    "## Hypotheses as functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0950f0a4-bd3f-484b-aabc-cd0cd2635dc7",
   "metadata": {},
   "source": [
    "Remember how we made `display=\"lambda: %s\"` in the definition of MyHypothesis? That stated that a hypothesis was not a function of any arguments since the `lambda` has no arguments. You may have noticed that when a hypothesis is converting to a string (for printing or evaling) it acquires this additional `lambda` on the outside, indicating that the hypothesis was a function of no arguments, or a [thunk](https://en.wikipedia.org/wiki/Thunk). \n",
    "\n",
    "Here is a new listing where a class like MyHypothesis requires an argument. Now, when it renders, it comes with a `lambda x` in front, rather than just a `lambda`. There are two other primary changes: the grammar now has to allow the argument (`x`) to be produced in expressions, and the `datum.input` has to provide an argument, which gets bound to `x` when the function is evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d5121-16e5-480e-a2a6-5de6c8678f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## \n",
    "## Define a grammar\n",
    "######################################## \n",
    "\n",
    "from LOTlib3.Grammar import Grammar\n",
    "grammar = Grammar(start='EXPR')\n",
    "\n",
    "grammar.add_rule('EXPR', '(%s + %s)', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', '(%s * %s)', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', '(float(%s) / float(%s))', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', '(-%s)', ['EXPR'], 1.0)\n",
    "\n",
    "# Now define how the grammar uses x. The string 'x' must\n",
    "# be the same as used in the args below\n",
    "grammar.add_rule('EXPR', 'x', None, 1.0) \n",
    "\n",
    "for n in range(0,10):\n",
    "    grammar.add_rule('EXPR', str(n), None, 10.0/((n+1)**2))\n",
    "\n",
    "from math import log\n",
    "from LOTlib3.Hypotheses.LOTHypothesis import LOTHypothesis\n",
    "\n",
    "######################################## \n",
    "## Define the hypothesis\n",
    "######################################## \n",
    "\n",
    "# define a \n",
    "class MyHypothesisX(LOTHypothesis):\n",
    "    def __init__(self, **kwargs):\n",
    "        LOTHypothesis.__init__(self, grammar=grammar, display=\"lambda x: %s\", **kwargs)\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        try:\n",
    "            # try to do it from the superclass\n",
    "            return LOTHypothesis.__call__(self, *args)\n",
    "        except ZeroDivisionError:\n",
    "            # and if we get an error, return nan\n",
    "            return float(\"nan\")\n",
    "\n",
    "    def compute_single_likelihood(self, datum):\n",
    "        if self(*datum.input) == datum.output:\n",
    "            return log((1.0-datum.alpha)/100. + datum.alpha)\n",
    "        else:\n",
    "            return log((1.0-datum.alpha)/100.)\n",
    "\n",
    "######################################## \n",
    "## Define the data\n",
    "######################################## \n",
    "\n",
    "from LOTlib3.DataAndObjects import FunctionData\n",
    "\n",
    "# Now our data takes input x=3 and maps it to 12\n",
    "# What could the function be?\n",
    "data = [ FunctionData(input=[3], output=12, alpha=0.95) ]\n",
    "\n",
    "######################################## \n",
    "## Actually run\n",
    "######################################## \n",
    "from LOTlib3.Samplers.MetropolisHastings import MetropolisHastingsSampler\n",
    "from LOTlib3.TopN import TopN\n",
    "\n",
    "tn = TopN(N=10) \n",
    "h0 = MyHypothesisX()\n",
    "\n",
    "for h in MetropolisHastingsSampler(h0, data, steps=10000):\n",
    "    tn << h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65fdd1-90ef-4890-bcfd-ac87700f7d41",
   "metadata": {},
   "source": [
    "Why does this matter? Well now instead of just explaining the data we saw, we can use the hypothesis to generalize to *new*, unseen data. For instance, we can take each hypothesis and see what it has to say about other numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bf5637-834d-466b-a249-b3e15a30d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = TopN(N=10) \n",
    "h0 = MyHypothesisX()\n",
    "\n",
    "for h in MetropolisHastingsSampler(h0, data, steps=10000):\n",
    "    tn << h\n",
    "\n",
    "# And now for each top N hypothesis, we'll see what it maps 0..9 to \n",
    "for h in tn.get_all(sorted=True):\n",
    "    print(h.posterior_score, h, list(map(h, range(0,10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff638d9-19a8-457c-bb53-11760b91618f",
   "metadata": {},
   "source": [
    "Thus, we have taken a single data point and used it to infer a function that can *generalize* to new, unseen data or arguments. Note, though, that there is no requirement that `x` is used in each hypothesis. (If we wanted that, a LOTlib3ian way to do it would be to modify the prior to assign trees that don't use `x` to have `-Infinity` log prior). \n",
    "\n",
    "Just for fun here, let's take the posterior predictive and see how likely we are to generalize this function to each other number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bdc056-0b7d-4feb-85a9-5c223a04215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's make a bunch of hypotheses\n",
    "from LOTlib3.TopN import TopN\n",
    "\n",
    "tn = TopN(1000) \n",
    "\n",
    "h0 = MyHypothesisX()\n",
    "for h in MetropolisHastingsSampler(h0, data, steps=100000): # run more steps\n",
    "    tn.add(h)\n",
    "\n",
    "# store these in a list (tn.get_all is defaultly a generator)\n",
    "hypotheses = list(tn.get_all())\n",
    "\n",
    "# Compute the normalizing constant\n",
    "from LOTlib3.Miscellaneous import logsumexp\n",
    "z = logsumexp([h.posterior_score for h in hypotheses])\n",
    "\n",
    "## Now compute a matrix of how likely each input is to go\n",
    "## to each output\n",
    "M = 20 # an MxM matrix of values\n",
    "import numpy\n",
    "\n",
    "# The probability of generalizing\n",
    "G = numpy.zeros((M,M))\n",
    "\n",
    "# Now add in each hypothesis' predictive\n",
    "for h in hypotheses:\n",
    "    # the (normalized) posterior probability of this hypothesis\n",
    "    p = numpy.exp(h.posterior_score - z)\n",
    "\n",
    "    for x in range(M):\n",
    "        output = int(h(x))\n",
    "\n",
    "        # only keep those that are in the right range\n",
    "        if 0 <= output < M:\n",
    "            G[x][int(output)] += p\n",
    "\n",
    "# And show the output\n",
    "print(numpy.array_str(G, precision=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5de8ddc-92e1-4eb0-964b-4849f15dd032",
   "metadata": {},
   "source": [
    "As you can see from the complexity of this array which shows the probabilities for different outputs given different inputs, observing that some (unseen) function maps 3->12 gives rise to nontrivial beliefs about the function underlying this transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b1c989-c1dc-474b-92c9-584f529a877a",
   "metadata": {},
   "source": [
    "# Lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd43e3-b48a-4fba-be30-9d9d5f3e119a",
   "metadata": {},
   "source": [
    "The power of this kind of representation comes not only from an ability to learn such simple functions, but to also learn functions with new kinds of abstractions. In programming languages, the simplest kind of abstraction is a variable--a value that is stored for later use. The variable `x` is created above on the level of a LOTHypothesis, but where things get more interesting is when the lower down values in a grammar can be used to define variables. Let's look at a grammar with two additional pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be04851-8311-4af4-9e88-2a58a1793ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LOTlib3.Grammar import Grammar\n",
    "grammar = Grammar(start='EXPR')\n",
    "\n",
    "grammar.add_rule('EXPR', '(%s + %s)', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', '(%s * %s)', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', '(float(%s) / float(%s))', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', '(-%s)', ['EXPR'], 1.0)\n",
    "\n",
    "grammar.add_rule('EXPR', 'x', None, 1.0) \n",
    "\n",
    "for n in range(1,10):\n",
    "    # We'll make these lower probability so we can see more lambdas below\n",
    "    grammar.add_rule('EXPR', str(n), None, 5.0/n**2)\n",
    "\n",
    "# And allow lambda abstraction\n",
    "# First we define the application of a new nonterminal, FUNC, to a term EXPR\n",
    "grammar.add_rule('EXPR', '(%s)(%s)', ['FUNC', 'EXPR'], 1.0)\n",
    "# Here, FUNC should be thought of as a function\n",
    "grammar.add_rule('FUNC', 'lambda', ['EXPR'], 1.0, bv_type='EXPR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff61230-6b55-4d9a-9501-5a575c0043fa",
   "metadata": {},
   "source": [
    "Here, `lambda` is a special LOTlib3 keyword that *introduces a bound variable* with a unique name in expanding the <ARGUMENT>s. In other words, when the grammar happens to sample a rule whose <FUNCTION> is `'lambda'`, it creates a new variable name, allows `bv_type` to expand to this variable, expands the <ARGUMENTS> to `lambda` (here, `EXPR`), and then removes the rule from the grammar. Let's look at some productions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57764fc1-dce0-4747-bb34-4958acee5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    print(grammar.generate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca172e2f-d58c-40f8-95df-e282c2ffc267",
   "metadata": {},
   "source": [
    "Now some of the trees contain `lambda` expressions, which bind a variable (defaultly rendered as `y1`). The variable `y1` can only be used below its corresponding lambda, making the grammar in LOTlib3 technically not context-free, but very weakly context-sensitive. The variables like `y1` are called **bound variables** in LOTlib3. Note that they are numbered by their height in the tree, making them unique to the nodes below, but neither sequential, nor unique in the whole tree (underlyingly, they have unique names no matter what, but not when rendered into strings). \n",
    "\n",
    "These bound variables count towards the prior (when using `grammar.log_probability`) in exactly the way they should: when a nonterminal (specified in `bv_type`) can expand to a given bound variable, that costs probability, and other expansions must lose probability. The default in LOTlib3 is to always renormalize the probabilities specified. Note that in the `add_rule` command, we can change the probability that a EXPR->yi rule has by passing in a bv_p argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841b2b9-8c7c-4ec1-9b9e-2a2314af4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make using yi 10x more likely than before\n",
    "grammar.add_rule('FUNC', 'lambda', ['EXPR'], 1.0, bv_type='EXPR', bv_p=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a6aac-7253-453c-9435-af4368929f16",
   "metadata": {},
   "source": [
    "Lambdas like these play the role of variable declarations in a normal programming language. But note that the variables aren't guaranteed to be useful. In fact, very often variables are stupid, as in the expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0b425-e675-4299-8b5f-f7e133351a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "(lambda y1: y1)((1 * 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe7040-7e3f-4ec4-afc9-61fa3b66846c",
   "metadata": {},
   "source": [
    "where the lambda defines a variable that is used immediately without modification. This expression is therefore equivalent to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b224a0-5105-4540-ac6f-2ce052cc063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 * 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb34cf-453c-4532-97f2-9960a6440799",
   "metadata": {},
   "source": [
    "in terms of its function, but not in terms of its prior. \n",
    "\n",
    "We can also change the name that bound variables get by setting `bv_prefix`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14c4a0-e811-4e25-97fe-0180faf20239",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar.add_rule('FUNC', 'lambda', ['EXPR'], 1.0, bv_type='EXPR', bv_prefix='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986382cc-f250-49d8-bfb3-d77926efc1b9",
   "metadata": {},
   "source": [
    "will make bound variables named `v1`, `v2`, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0cd65-09c0-48ce-8f1a-da6e8b72d726",
   "metadata": {},
   "source": [
    "## Here's where things get crazy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3be241-ca1b-49b2-8eb1-b0394cb9e696",
   "metadata": {},
   "source": [
    "Of course, the true art of lambdas is not just that they can define variables, but that the variables themselves can be functions! This corresponds to *function declarations* in ordinary programming languages. If this is foreign or weird, I'd suggest reading [The Structure and Interpretation of Computer Programs](https://mitpress.mit.edu/sicp/). \n",
    "\n",
    "To define lambdas as functions, we only need to specify a `bv_args` list in the `lambda` declaration. `bv_args` is the type of arguments that are passed to each use of a bound variable each time it is used. But... then we have a problem of needing to bind that variable to something. If `yi` is itself a function of an EXPR, then its argument *also* has to be a function. That requires two lambdas. Here's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b713284d-23ed-4614-828f-27c4fa9f8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LOTlib3.Grammar import Grammar\n",
    "grammar = Grammar(start='EXPR')\n",
    "\n",
    "grammar.add_rule('EXPR', '(%s + %s)', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', '(%s * %s)', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', '(float(%s) / float(%s))', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', '(-%s)', ['EXPR'], 1.0)\n",
    "\n",
    "grammar.add_rule('EXPR', 'x', None, 1.0)  \n",
    "\n",
    "for n in xrange(1,10):\n",
    "    grammar.add_rule('EXPR', str(n), None, 5.0/n**2)\n",
    "\n",
    "# Allow ourselves to define functions. This means creating a bound \n",
    "# variable that can be bound to a FUNC. Where, the bound variable\n",
    "# is defined (here, FUNCDEF) we are allowed to use it. \n",
    "grammar.add_rule('EXPR', '((%s)(%s))',  ['FUNCDEF', 'FUNC'], 1.0)\n",
    "\n",
    "# The function definition has a bound variable who can be applied as\n",
    "# a function, whose arguments are an EXPR (set by the type of the FUNC above)\n",
    "# and whose name is F, and who when applied to an EXPR returns an EXPR\n",
    "# We'll also set bv_p here. Feel free to play with it and see what that does. \n",
    "grammar.add_rule('FUNCDEF', 'lambda', ['EXPR'], 1.0, bv_type='EXPR', bv_args=['EXPR'], bv_prefix='F')\n",
    "\n",
    "# and we have to say what a FUNC is. It's a function (lambda) from an EXPR to an EXPR\n",
    "grammar.add_rule('FUNC', 'lambda', ['EXPR'], 1.0, bv_type='EXPR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a8b411-7a35-4f6a-9447-68b4f73e9aa1",
   "metadata": {},
   "source": [
    "Let's look at some hypotheses. Here, we'll show only those that use `F1` as a function (thus contain the string `\"F1(\"`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c4c73-8fd6-46e9-aec5-5d75fbac23e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "for _ in xrange(50000):\n",
    "    t = grammar.generate()\n",
    "    if re.search(r\"F1\\(\", str(t)):\n",
    "        print t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e39a1be-fefe-4a28-9c21-f8c7fb3d2b35",
   "metadata": {},
   "source": [
    "For instance, this code might generate the following expression, which is obscure, though acceptable, python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eadee3d-8976-4785-9c17-5c9301c04f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "((lambda F1: F1(x+1))(lambda y1: y1+3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2bc25-9de1-4b7f-856b-50334bf7c3cc",
   "metadata": {},
   "source": [
    "Here, we have define a variable `F1` that really represents the *function* `lambda y1: y1+3`. The value that is returned is the value of applying `F1` to the overall hypothesis value `x` plus `1`. Note that LOTlib3 here has correctly used `F1` in a context where an EXPR is needed (due to `bv_type='EXPR'` on `FUNCDEF`). It knows that the argument to `F1` is also an EXPR, which here happens to be expanded to `x+1`. It also knows that `F1` is itself a function, and it binds this function (through the outermost apply) to `lambda y1: y1+3`. LOTlib3 knows that `F1` can only be used in the left hand side of this apply, and `y1` can only be used on the right. This holds even if multiple bound variables of different types are generated. \n",
    "\n",
    "This ability to define functions provides some of the most interesting learning dynamics for the model. A nice example is provided in LOTlib3.Examples.Magnetism, where learners take data and learn predicates classifying observable objects into two kinds, as well as laws stated over those kinds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249770a5-8f8b-4ef9-ab73-59b48330075a",
   "metadata": {},
   "source": [
    "## Recursive functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6218f95d-cf35-4300-a02f-3fe70bfdf694",
   "metadata": {},
   "source": [
    "Well that's wonderful, but what if we want a function to refer to *itself*? This is common in programming languages in the form of [recursive](https://en.wikipedia.org/wiki/Recursion_%28computer_science%29) definitions. This takes a little finagling in the LOTlib3 internals (through ambitious use of the [Y-combinator](https://en.wikipedia.org/wiki/Fixed-point_combinator#Fixed_point_combinators_in_lambda_calculus)) which you don't have to worry about. There is a class that implements recursion straightforwardly: `RecursiveLOTHypothesis`. Internally, hypothesis of this type always have an argument (defaultly called `recurse_`) which binds to themselves! \n",
    "\n",
    "Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1dc920-d663-4bf2-8ef0-a6d424962705",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## \n",
    "## Define the grammar\n",
    "######################################## \n",
    "\n",
    "from LOTlib3.Grammar import Grammar\n",
    "grammar = Grammar(start='EXPR')\n",
    "\n",
    "# for simplicity, two operations\n",
    "grammar.add_rule('EXPR', '(%s + %s)', ['EXPR', 'EXPR'], 1.0)\n",
    "grammar.add_rule('EXPR', '(%s * %s)', ['EXPR', 'EXPR'], 1.0)\n",
    "\n",
    "# we'll just allow two terminals for simplicity\n",
    "# We have to upweight them a little to keep things well-defined\n",
    "grammar.add_rule('EXPR', 'x', None, 10.0) \n",
    "grammar.add_rule('EXPR', '1', None, 10.0) \n",
    "\n",
    "# If we're going to allow recursion, we better have a base case\n",
    "# But this probably requires an \"if\" statement. LOTlib3's \"if_\" \n",
    "# primitive will do the trick\n",
    "grammar.add_rule('EXPR', '(%s if %s else %s)', ['EXPR', 'BOOL', 'EXPR'], 1.0)\n",
    "\n",
    "# and we need to define a boolean. For now, let's just check\n",
    "# if x=1\n",
    "grammar.add_rule('BOOL', 'x==1', None, 1.0)\n",
    "\n",
    "# and the recursive operation -- I am myself a function\n",
    "# from EXPR to EXPR, so recurse should be as well\n",
    "grammar.add_rule('EXPR', 'recurse_', ['x-1'], 1.0) \n",
    "\n",
    "######################################## \n",
    "## Define the hypothesis\n",
    "######################################## \n",
    "from LOTlib3.Hypotheses.RecursiveLOTHypothesis import RecursiveLOTHypothesis\n",
    "\n",
    "class MyRecursiveHypothesis(RecursiveLOTHypothesis):\n",
    "    def __init__(self, **kwargs):\n",
    "        RecursiveLOTHypothesis.__init__(self, grammar=grammar, display=\"lambda recurse_, x: %s\", **kwargs)\n",
    "\n",
    "######################################## \n",
    "## Look at some examples\n",
    "######################################## \n",
    "import re\n",
    "from LOTlib3.Eval import RecursionDepthException\n",
    "\n",
    "for _ in range(50000):\n",
    "    h = MyRecursiveHypothesis()\n",
    "\n",
    "    # Now when we call h, something funny may happen: we may get\n",
    "    # an exception for recursing too deep. If this happens for some \n",
    "    # reasonable xes, let's not print the hypothesis -- it must not \n",
    "    # be well-defined\n",
    "    try:\n",
    "        # try our function out\n",
    "        values = map(h, range(1,10))\n",
    "    except RecursionDepthException:\n",
    "        continue\n",
    "\n",
    "    # if we succeed, let's only show hypotheses that use recurse:\n",
    "    if re.search(r\"recurse_\\(\", str(h)):\n",
    "        print(h) \n",
    "        print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933621df-20f0-499a-b64a-10633096fd17",
   "metadata": {},
   "source": [
    "Note that there is nothing special about the `recurse_` name: it can be changed by setting `recurse=...` in `RecursiveLOTHypothesis.__init__`, but then the name should also be changed in the grammar. In this tutorial, we have only looked at defining the grammar, not in inferring recursive hypotheses. LOTlib3.Examples.Number is an example of learning a genuinely recursive function from data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
